#!/usr/bin/env python
from __future__ import print_function
import argh
import bitmath
import boto3
import datetime
import requests
import sys
import os

from filecache import filecache
from first import first
from pprint import pprint
from sparklines import sparklines
from tabulate import tabulate


@filecache(24 * 60 * 60)
def rds_metadata():
    url = 'http://www.ec2instances.info/rds/instances.json'
    return requests.get(url).json()


@filecache(24 * 60 * 60)
def rds_offers(region):
    url = 'https://pricing.{0}.amazonaws.com/offers/v1.0/aws/AmazonRDS/current/index.json'.format(region)
    return requests.get(url).json()


@filecache(24 * 60 * 60)
def get_metric_datapoints(instance_identifier,
                          metric_name,
                          unit,
                          days_to_check):
    seconds_in_one_day = 86400  # used for granularity

    now = datetime.datetime.now()
    past = datetime.timedelta(days=days_to_check)

    client = boto3.client('cloudwatch')
    response = client.get_metric_statistics(
        Namespace='AWS/RDS',
        Dimensions=[
            {
                'Name': 'DBInstanceIdentifier',
                'Value': instance_identifier,
            },
        ],
        MetricName=metric_name,
        StartTime=now - past,
        EndTime=datetime.datetime.now(),
        Period=seconds_in_one_day,
        Statistics=[
            'Average'
        ],
        Unit=unit
    )
    return [point['Average'] for point in response.get('Datapoints', [])]


@filecache(24 * 60 * 60)
def get_db_instances():
    client = boto3.client('rds')
    paginator = client.get_paginator('describe_db_instances')
    page_iterator = paginator.paginate()

    rds_instances = []
    for page in page_iterator:
        for rds_instance in page['DBInstances']:
            resource = rds_instance['DBInstanceArn']
            response = client.list_tags_for_resource(ResourceName=resource)
            rds_instance['Tags'] = response['TagList']
            rds_instances.append(rds_instance)
    return rds_instances


@filecache(24 * 60 * 60)
def get_associated_offer(instance_identifier, instance_type, engine, region, is_multi_az):
    engine_map = {
        'mariadb': 'MariaDB',
        'mysql': 'MySQL',
        'postgres': 'PostgreSQL',
    }
    location_map = {
        'ap-south-1': 'Asia Pacific (Mumbai)',
        'ap-northeast-2': 'Asia Pacific (Seoul)',
        'ap-southeast-1': 'Asia Pacific (Singapore)',
        'ap-southeast-2': 'Asia Pacific (Sydney)',
        'ap-northeast-1': 'Asia Pacific (Tokyo)',
        'ca-central-1': 'Canada (Central)',
        'eu-central-1': 'EU (Frankfurt)',
        'eu-west-1': 'EU (Ireland)',
        'eu-west-2': 'EU (London)',
        'sa-east': 'South America (Sao Paulo)',
        'us-east-1': 'US East (N. Virginia)',
        'us-east-2': 'US East (Ohio)',
        'us-west-1': 'US West (N. California)',
        'us-west-2': 'US West (Oregon)',
    }

    database_engine = engine_map[engine]
    location = location_map[region]
    deployment_option = 'Multi-AZ' if is_multi_az else 'Single-AZ'

    offers = rds_offers(region)
    potential_products = []
    for sku, product in offers.get('products', {}).items():
        product_location = product['attributes'].get('location', None)
        product_instance_type = product['attributes'].get('instanceType', None)
        product_database_engine = product['attributes'].get('databaseEngine', None)
        product_deployment_option = product['attributes'].get('deploymentOption', 'Single-AZ')

        if product_location is None:
            continue
        if product_instance_type is None:
            continue
        if product_database_engine is None:
            continue

        if product_location != location:
            continue

        if product_instance_type != instance_type:
            continue

        if product_database_engine != database_engine:
            continue

        if product_deployment_option != deployment_option:
            continue

        potential_products.append(product)

    if len(potential_products) != 1:
        print('Error getting product for {0}'.format(instance_identifier))
        print('Number of potential matching products: {0}'.format(len(potential_products)))
        pprint(potential_products)
        sys.exit(1)

    product = potential_products[0]
    terms = offers.get('terms', {}).get('Reserved', {}).get(product['sku'], None)
    if terms is None:
        print('Error getting terms for product')
        sys.exit(1)

    return product, terms


def calculate_offerings(offer_terms):
    hours_per_year = 8760
    offerings = {}
    for sku, offer_term in offer_terms.items():
        length = offer_term['termAttributes']['LeaseContractLength']
        description = offer_term['termAttributes']['PurchaseOption']
        description = '{0}-{1}'.format(length, description)
        per_year_costs = 0.00
        for pd_sku, price_dimension in offer_term['priceDimensions'].items():
            if price_dimension['description'] == 'Upfront Fee':
                if length == '3yr':
                    per_year_costs += float(price_dimension['pricePerUnit']['USD']) / 3
                else:
                    per_year_costs += float(price_dimension['pricePerUnit']['USD'])
            else:
                per_year_costs += float(price_dimension['pricePerUnit']['USD']) * hours_per_year
        offerings[description.lower().replace(' ', '-')] = per_year_costs
    return offerings


def check_cpu(rds_instance, reservation_type, threshold=25, days_to_check=90):
    name = rds_instance['DBInstanceIdentifier']
    client = boto3.client('cloudwatch')
    datapoints = get_metric_datapoints(name,
                                       'CPUUtilization',
                                       'Percent',
                                       days_to_check)
    avg = float(sum(datapoints)) / len(datapoints)

    datapoints.append(100)
    reserve = False
    if avg < threshold:
        reserve = True

    offerings = calculate_offerings(rds_instance['offer_terms'])
    cost = offerings[reservation_type]

    return [name, rds_instance['DBInstanceClass'], cost, avg, reserve, first(sparklines(datapoints))]


def check_disk(rds_instance, reservation_type, threshold=25, days_to_check=90):
    name = rds_instance['DBInstanceIdentifier']
    total_disk = bitmath.GiB(float(rds_instance['AllocatedStorage']))
    client = boto3.client('cloudwatch')
    datapoints = get_metric_datapoints(name,
                                       'FreeStorageSpace',
                                       'Bytes',
                                       days_to_check)
    datapoints = [100 - (bitmath.Byte(d) / total_disk) * 100 for d in datapoints]
    avg = float(sum(datapoints)) / len(datapoints)

    datapoints.append(100)
    reserve = False
    if avg < threshold:
        reserve = True

    offerings = calculate_offerings(rds_instance['offer_terms'])
    cost = offerings[reservation_type]

    return [name, rds_instance['DBInstanceClass'], cost, avg, reserve, first(sparklines(datapoints))]


def check_memory(rds_instance, reservation_type, threshold=25, days_to_check=90):
    name = rds_instance['DBInstanceIdentifier']
    total_memory = bitmath.GiB(float(rds_instance['meta']['memory']))
    client = boto3.client('cloudwatch')
    datapoints = get_metric_datapoints(name,
                                       'FreeableMemory',
                                       'Bytes',
                                       days_to_check)
    datapoints = [100 - (bitmath.Byte(d) / total_memory) * 100 for d in datapoints]
    avg = float(sum(datapoints)) / len(datapoints)

    datapoints.append(100)
    reserve = False
    if avg < threshold:
        reserve = True

    offerings = calculate_offerings(rds_instance['offer_terms'])
    cost = offerings[reservation_type]

    return [name, rds_instance['DBInstanceClass'], cost, avg, reserve, first(sparklines(datapoints))]


def get_associated_metadata(rds_instance, metadata):
    engine_map = {
        'mariadb': 'MariaDB',
        'mysql': 'MySQL',
        'postgres': 'PostgreSQL',
    }

    database_engine = engine_map[rds_instance['Engine']]

    for m in metadata:
        region_pricing = m['pricing'].get(rds_instance['region'], None)
        if region_pricing is None:
            continue

        if region_pricing.get(database_engine, None) is None:
            continue

        if m['instanceType'] != rds_instance['DBInstanceClass']:
            continue
        return m


@argh.arg('-C', '--check', choices=['cpu', 'disk', 'memory'])
@argh.arg('-K', '--tag-key')
@argh.arg('-V', '--tag-value')
@argh.arg('-T', '--threshold')
@argh.arg('-D', '--days-to-check')
@argh.arg('-N', '--no-sparklines')
@argh.arg('-S', '--sort-by', choices=['name', 'percentage', 'reserve'])
@argh.arg('-R', '--reservation-type', choices=['1yr-no-upfront', '1yr-partial-upfront', '1yr-all-upfront', '3yr-partial-upfront', '3yr-all-upfront'])
def main(check='disk',
         no_sparklines=False,
         tag_key='environment',
         tag_value='production',
         threshold=25,
         days_to_check=90,
         sort_by='name',
         reservation_type='1yr-no-upfront'):
    metric_funcs = {
        'cpu': check_cpu,
        'disk': check_disk,
        'memory': check_memory,
    }

    metadata = rds_metadata()

    rds_instances = []
    for rds_instance in get_db_instances():
        instance_identifier = rds_instance['DBInstanceIdentifier']
        instance_type = rds_instance['DBInstanceClass']
        engine = rds_instance['Engine']
        region = rds_instance['AvailabilityZone'][:-1]
        is_multi_az = rds_instance.get('MultiAZ', False)

        product, terms = get_associated_offer(instance_identifier, instance_type, engine, region, is_multi_az)
        rds_instance['offer_product'] = product
        rds_instance['offer_terms'] = terms
        rds_instance['region'] = rds_instance['AvailabilityZone'][:-1]
        rds_instance['meta'] = get_associated_metadata(rds_instance, metadata)

        for tag in rds_instance['Tags']:
            if tag['Key'] == tag_key and tag['Value'] == tag_value:
                rds_instances.append(rds_instance)

    table = []
    for rds_instance in rds_instances:
        metrics = metric_funcs[check](rds_instance, reservation_type, threshold, days_to_check)
        if no_sparklines:
            del metrics[-1]
        table.append(metrics)

    if sort_by == 'percentage':
        table.sort(key=lambda x: x[1])
    if sort_by == 'reserve':
        table.sort(key=lambda x: x[2])

    headers = ['instance identifier', 'instance type', 'percentage', 'reserve', 'sparklines']
    if no_sparklines:
        del headers[-1]
    print(tabulate(table, headers=headers))


if __name__ == '__main__':
    argh.dispatch_command(main)
